# ===================== CGTC-Core + PCGrad (Grad-ratio Guard) TEST =====================
# 目标：Core 控制语义 + Grad-ratio Guard + PCGrad 仅在 cooling 期间生效（由 trainer 逻辑保证）

train:
  epochs: 200
  lr: 1.0e-4
  weight_decay: 1.0e-4
  eta_min: 1.0e-5
  grad_clip: 1.0
  device: "cuda:0"
  log_interval: 1
  # 如果你希望从 epoch1 就三者共训（不走 warm stage），打开：
  # start_stage: "warmup"

loss:
  channel_weights: [1.0, 0.5, 0.5, 0.5]
  phys_alpha: 0.5

guard:
  disable: false
  gradcos_interval: 10
  layers: ["auto"]
  eps: 1.0e-8
  # 补全 default.yaml 中可能出现的字段（你的 trainer 也兼容）
  reverse_hierarchy: false
  no_rollback: false
  no_freeze_teacher: false

pinn:
  use: true
  k: 24
  samples: 4096
  rho: 1.0
  nu_eff: 0.003
  lambda_mom: 0.001

  # --- schedule ---
  warmup: 100
  ramp: 40
  smooth_epochs: 12
  lmax: 0.02
  lmin: 0.002
  schedule: "cosine"
  mode: "down"

  # --- Core: Grad-ratio guard ---
  guard_metric: "grad"

  # τ = ||g_phys|| / (||g_sup|| + eps) 的上限
  # 起点建议 3~8；这里取 5.0 作为 test 默认
  max_loss_ratio: 5.0

  # 你当前 trainer 里 vs_spatial 仍是 “cap” 语义（且尚未完全切到 grad 口径）
  # 为避免误用，测试版先置 0.0
  max_ratio_vs_spatial: 0.0

  # --- CGTC control knobs ---
  guard_blend_reset: 0.65
  guard_backoff: 0.4
  guard_adapt_min: 0.35
  guard_blend_decay: 0.5
  guard_cooldown: 3
  guard_gamma: 0.7
  cooling_window: 4

  # --- Core: 关闭 enhance 的 lcont_mult 自适应 ---
  adapt: false
  lcont_min_mult: 0.5
  lcont_max_mult: 1.8
  lcont_mult_init: 1.0
  up_rate: 1.05
  down_rate: 0.985
  adapt_after: 121
  adapt_interval: 3

  # --- targets / calibration ---
  div_target: 0.1
  div_tol: 0.20
  auto_calib: false
  calib_mult: 0.6
  boost_epochs: 30

teacher:
  use: true
  ema_beta: 0.995
  # 可选：如果你后面在 default 里加了 ema_beta_balance，这里也可补齐
  # ema_beta_balance: 0.995
  max_w: 0.20
  decay_eps: 200
  use_best_warm: true
  freeze_after_warm: true

  # teacher guard 仍是 loss-ratio 语义（与你 trainer 一致）
  max_ratio: 0.35

  spatial:
    use: true
    max_deg: 6.0
    weight: 0.05
    max_ratio: 0.20
    gamma: 0.75
    cooling_window: 3
    recover_step: 0.1

p_grad:
  release_epoch: 130
  scale: 0.05

robust:
  subsample_ratio: 1.0
  coord_noise_sigma: 0.0
  field_noise_sigma: 0.0
  occlusion_ratio: 0.0
  occlusion_min: 0.05
  occlusion_max: 0.25
  scale_factor: 1.0
  apply_to_val: false
  apply_to_test: false
  ood_rotation:
    enabled: false
    max_deg: 45.0

curriculum:
  split: 0.8

# Core：关闭 mixed（保持监督口径干净）
mixed:
  use: false
  alpha_start: 1.0
  alpha_end: 0.8
  decay_start: 1
  decay_epochs: 200
  schedule: "cosine"
  wall_sigma: 0.10

# Core：关闭通道自适应
adapt:
  enable: false
  start_epoch: 1
  ema_beta: 0.95
  tau: 1.0
  group_velocity: true
  w_min: 0.5
  w_max: 2.0

# ✅ Core + PCGrad（战术手术）：仅在 cooling_active 时由 trainer 启用
mo:
  solver: "pcgrad"         # "sum" / "pcgrad" / "cagrad"
  cagrad_alpha: 0.5        # 补全字段（即使不用也无害）
  pcgrad_proj: "conflict"
  pcgrad_retain_low: true

early:
  patience: 15
  delta: 1.0e-4

save:
  ok_margin: 0.40
  metric: "val_loss"

models:
  backbone:
    name: "pointnet2pp_ssg"
    args:
      out_channels: 4
      p_drop: 0.3
      npoint1: 2048
      npoint2: 512
      npoint3: 128
      k1: 16
      k2: 16
      k3: 16
      c1: 128
      c2: 256
      c3: 512
      groups_gn: 32
      use_pe: false
      use_msg: false
      in_ch0: 0
      p_head_ch: [128, 64]
      uvw_head_ch: [128, 64]
      use_graph_refine: true
      refine_k: 16
      refine_layers: 2
      refine_hidden: 128
      refine_residual: true
