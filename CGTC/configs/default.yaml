# =====================  CGTC Default (Grad-ratio Guard, Core+Enhance knobs kept) =====================
# Compatible with your current trainer.py (supports pinn.guard_metric = "grad" / "loss").

train:
  epochs: 250
  lr: 1.0e-4
  weight_decay: 1.0e-4
  eta_min: 1.0e-5
  grad_clip: 1.0
  device: "cuda:0"
  log_interval: 1

loss:
  channel_weights: [1.0, 0.5, 0.5, 0.5]
  phys_alpha: 0.5

guard:
  disable: false
  gradcos_interval: 10
  layers: ["auto"]
  eps: 1.0e-8
  reverse_hierarchy: false
  no_rollback: false
  no_freeze_teacher: false
  # Cosine-based trust-region control for PINN gradients
  cos_control: true
  cos_trust: 0.2
  cos_surgery: 0.0
  cos_freeze: -0.3
  cos_scale_min: 0.2
  cos_surgery_solver: "pcgrad"
  # When cos_control is enabled, PINN cooling triggers follow cos stages.

pinn:
  use: true
  k: 24
  samples: 4096
  rho: 1.0
  nu_eff: 0.003
  lambda_mom: 0.001

  # --- schedule / stage semantics ---
  warmup: 100
  ramp: 40
  smooth_epochs: 12
  lmax: 0.02
  lmin: 0.002
  schedule: "cosine"
  mode: "down"

  # --- Guard metric ---
  # "loss": old loss-ratio guard (L_phys / L_sup)
  # "grad": new grad-ratio guard (||g_phys|| / (||g_sup||+eps))
  guard_metric: "grad"

  # For grad-guard: tau = ||g_phys||/(||g_sup||+eps) upper bound
  # Start with 5.0; tune in [3, 8] depending on how aggressive you want the controller.
  max_loss_ratio: 5.0

  # NOTE: In your current grad-guard trainer, "vs_spatial" extra-cap is NOT used yet.
  # Set to 0.0 to avoid misleading semantics. (You can re-enable after upgrading trainer.)
  max_ratio_vs_spatial: 0.0

  # --- CGTC control knobs ---
  guard_blend_reset: 0.65
  guard_backoff: 0.4
  guard_adapt_min: 0.35
  guard_blend_decay: 0.5
  guard_cooldown: 3
  guard_gamma: 0.7
  cooling_window: 4

  # --- Enhance: lambda_cont multiplier adaptation ---
  adapt: true
  lcont_min_mult: 0.5
  lcont_max_mult: 1.8
  lcont_mult_init: 1.0
  up_rate: 1.05
  down_rate: 0.985
  adapt_after: 121
  adapt_interval: 3

  # --- Physics targets ---
  div_target: 0.1
  div_tol: 0.20
  auto_calib: true
  calib_mult: 0.6
  boost_epochs: 30

teacher:
  use: true
  ema_beta: 0.995
  # Optional: your trainer uses ema_beta_balance if present (fallback to ema_beta)
  # ema_beta_balance: 0.995
  max_w: 0.20
  decay_eps: 200
  # Present for completeness; trainer may not explicitly branch on it, but safe to keep.
  use_best_warm: true
  freeze_after_warm: true

  # teacher ratio guard (still loss-based in your trainer)
  max_ratio: 0.35

  spatial:
    use: true
    max_deg: 6.0
    weight: 0.05
    max_ratio: 0.20
    gamma: 0.75
    cooling_window: 3
    recover_step: 0.1

# Pressure gradient hook settings
p_grad:
  release_epoch: 130
  scale: 0.05

# Robustness knobs (usually applied in dataset pipeline / wo.py; trainer keeps metadata)
robust:
  subsample_ratio: 1.0
  coord_noise_sigma: 0.0
  field_noise_sigma: 0.0
  occlusion_ratio: 0.0
  occlusion_min: 0.05
  occlusion_max: 0.25
  scale_factor: 1.0
  apply_to_val: false
  apply_to_test: false
  ood_rotation:
    enabled: false
    max_deg: 45.0

curriculum:
  split: 0.8

mixed:
  use: true
  alpha_start: 1.0
  alpha_end: 0.8
  decay_start: 1
  decay_epochs: 200
  schedule: "cosine"
  wall_sigma: 0.10

# Adaptive channel weights (supervised channels)
adapt:
  enable: true
  start_epoch: 1
  ema_beta: 0.95
  tau: 1.0
  group_velocity: true
  w_min: 0.5
  w_max: 2.0

# Multi-objective solver (only active during cooling in your trainer logic)
mo:
  solver: "sum"        # "sum" / "pcgrad" / "cagrad"
  cagrad_alpha: 0.5
  # The following are kept for CLI/YAML compatibility but may not be used in current trainer:
  pcgrad_proj: "conflict"
  pcgrad_retain_low: true

early:
  patience: 15
  delta: 1.0e-4

save:
  ok_margin: 0.40
  metric: "val_loss"

models:
  backbone:
    name: "pointnet2pp_ssg"
    args:
      out_channels: 4
      p_drop: 0.3
      npoint1: 2048
      npoint2: 512
      npoint3: 128
      k1: 16
      k2: 16
      k3: 16
      c1: 128
      c2: 256
      c3: 512
      groups_gn: 32
      use_pe: false
      use_msg: false
      in_ch0: 0
      p_head_ch: [128, 64]
      uvw_head_ch: [128, 64]
      use_graph_refine: true
      refine_k: 16
      refine_layers: 2
      refine_hidden: 128
      refine_residual: true
